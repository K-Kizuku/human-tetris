//
//  FacialExpressionManager.swift
//  human-tetris
//
//  Created by Kiro on 2025/08/17.
//

import ARKit
import Combine
import SwiftUI

// Ë°®ÊÉÖ„ÅÆÁ®ÆÈ°û„ÇíÂÆöÁæ©
enum FacialExpression: String, CaseIterable {
    case neutral = "‰∏≠Á´ã"
    case happy = "Âñú„Å≥"
    case angry = "ÊÄí„Çä"
    case surprised = "È©ö„Åç"
    case sad = "ÊÇ≤„Åó„Åø"

    var emoji: String {
        switch self {
        case .neutral: return "üòê"
        case .happy: return "üòä"
        case .angry: return "üò†"
        case .surprised: return "üò≤"
        case .sad: return "üò¢"
        }
    }

    /// Ë°®ÊÉÖ„Å´Âøú„Åò„Åü„ÉÜ„Éà„É™„ÇπËêΩ‰∏ãÈÄüÂ∫¶„ÅÆÂÄçÁéá
    /// „Éù„Ç∏„ÉÜ„Ç£„Éñ„Å™ÊÑüÊÉÖ„Åª„Å©ÈÅÖ„ÅèÔºà0.8ÂÄçÔºâ„ÄÅ„Éç„Ç¨„ÉÜ„Ç£„Éñ„Å™ÊÑüÊÉÖ„Åª„Å©ÈÄü„ÅèÔºà1.5ÂÄçÔºâ
    var dropSpeedMultiplier: Double {
        switch self {
        case .happy: return 0.8  // Âñú„Å≥Ôºö„ÇÜ„Å£„Åè„ÇäËêΩ‰∏ã
        case .neutral: return 1.0  // ‰∏≠Á´ãÔºöÈÄöÂ∏∏ÈÄüÂ∫¶
        case .surprised: return 1.2  // È©ö„ÅçÔºöÂ∞ë„ÅóÈÄü„Åè
        case .sad: return 1.3  // ÊÇ≤„Åó„ÅøÔºöÈÄü„Åè
        case .angry: return 1.5  // ÊÄí„ÇäÔºöÊúÄ„ÇÇÈÄü„Åè
        }
    }

    /// Ë°®ÊÉÖ„ÅÆÊÑüÊÉÖÁöÑ„Å™Ê•µÊÄßÔºà„Éù„Ç∏„ÉÜ„Ç£„Éñ/„Éç„Ç¨„ÉÜ„Ç£„ÉñÔºâ
    var emotionalPolarity: String {
        switch self {
        case .happy: return "„Éù„Ç∏„ÉÜ„Ç£„Éñ"
        case .neutral: return "‰∏≠Á´ã"
        case .surprised: return "‰∏≠Á´ã"
        case .sad: return "„Éç„Ç¨„ÉÜ„Ç£„Éñ"
        case .angry: return "„Éç„Ç¨„ÉÜ„Ç£„Éñ"
        }
    }
}

// Ë°®ÊÉÖË™çË≠ò„ÅÆÁµêÊûú„ÇíË°®„Åô„Éá„Éº„ÇøÊßãÈÄ†
struct FacialExpressionResult {
    let expression: FacialExpression
    let confidence: Float
    let timestamp: Date
}

protocol FacialExpressionManagerDelegate {
    func facialExpressionManager(
        _ manager: FacialExpressionManager, didDetectExpression result: FacialExpressionResult)
    func facialExpressionManager(_ manager: FacialExpressionManager, didEncounterError error: Error)
}

class FacialExpressionManager: NSObject, ObservableObject {
    @Published var currentExpression: FacialExpression = .neutral
    @Published var confidence: Float = 0.0
    @Published var isTracking = false
    @Published var isFaceDetected = false
    @Published var isARKitSupported = false

    var delegate: FacialExpressionManagerDelegate?

    private let sessionQueue = DispatchQueue(label: "facial.expression.session.queue")
    private var simulationTimer: Timer?

    // ARKitÈñ¢ÈÄ£
    private var arSession: ARSession?
    private var arConfiguration: ARFaceTrackingConfiguration?

    override init() {
        super.init()
        checkARKitSupport()
        print("FacialExpressionManager: Initialized with ARKit support: \(isARKitSupported)")
    }

    deinit {
        stopTracking()
    }

    private func checkARKitSupport() {
        isARKitSupported = ARFaceTrackingConfiguration.isSupported
        print("FacialExpressionManager: ARKit Face Tracking supported: \(isARKitSupported)")
    }

    func startTracking() {
        print("FacialExpressionManager: Starting tracking")

        sessionQueue.async { [weak self] in
            guard let self = self else { return }

            if self.isARKitSupported {
                self.startARKitTracking()
            } else {
                print("FacialExpressionManager: ARKit not supported, using simulation mode")
                DispatchQueue.main.async {
                    self.isTracking = true
                    self.isFaceDetected = true
                    self.startSimulation()
                }
            }
        }
    }

    func stopTracking() {
        print("FacialExpressionManager: Stopping tracking")

        sessionQueue.async { [weak self] in
            // ARKit„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÂÅúÊ≠¢
            self?.arSession?.pause()
            self?.arSession = nil

            // „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„Çø„Ç§„Éû„Éº„ÇíÂÅúÊ≠¢
            self?.simulationTimer?.invalidate()
            self?.simulationTimer = nil

            DispatchQueue.main.async {
                self?.isTracking = false
                self?.isFaceDetected = false
                self?.currentExpression = .neutral
                self?.confidence = 0.0
            }
        }
    }

    // MARK: - ARKit Face Tracking

    private func startARKitTracking() {
        guard ARFaceTrackingConfiguration.isSupported else {
            print("FacialExpressionManager: ARKit Face Tracking not supported")
            return
        }

        let configuration = ARFaceTrackingConfiguration()
        configuration.maximumNumberOfTrackedFaces = 1

        arSession = ARSession()
        arSession?.delegate = self

        DispatchQueue.main.async {
            self.arSession?.run(configuration)
            self.isTracking = true
        }

        print("FacialExpressionManager: ARKit Face Tracking started")
    }

    private func analyzeBlendShapes(_ blendShapes: [ARFaceAnchor.BlendShapeLocation: NSNumber]) -> (
        expression: FacialExpression, confidence: Float
    ) {
        // ÂêÑË°®ÊÉÖ„ÅÆÁâπÂæ¥ÁöÑ„Å™blendShape„ÇíÂàÜÊûê

        // Á¨ëÈ°î„ÅÆÊ§úÂá∫
        let mouthSmileLeft = blendShapes[.mouthSmileLeft]?.floatValue ?? 0.0
        let mouthSmileRight = blendShapes[.mouthSmileRight]?.floatValue ?? 0.0
        let _ = blendShapes[.cheekPuff]?.floatValue ?? 0.0  // Â∞ÜÊù•„ÅÆÊã°ÂºµÁî®
        let smileIntensity = (mouthSmileLeft + mouthSmileRight) / 2.0

        // ÊÄí„Çä„ÅÆÊ§úÂá∫
        let browDownLeft = blendShapes[.browDownLeft]?.floatValue ?? 0.0
        let browDownRight = blendShapes[.browDownRight]?.floatValue ?? 0.0
        let mouthFrownLeft = blendShapes[.mouthFrownLeft]?.floatValue ?? 0.0
        let mouthFrownRight = blendShapes[.mouthFrownRight]?.floatValue ?? 0.0
        let angerIntensity = (browDownLeft + browDownRight + mouthFrownLeft + mouthFrownRight) / 4.0

        // È©ö„Åç„ÅÆÊ§úÂá∫
        let browInnerUp = blendShapes[.browInnerUp]?.floatValue ?? 0.0
        let eyeWideLeft = blendShapes[.eyeWideLeft]?.floatValue ?? 0.0
        let eyeWideRight = blendShapes[.eyeWideRight]?.floatValue ?? 0.0
        let jawOpen = blendShapes[.jawOpen]?.floatValue ?? 0.0
        let surpriseIntensity = (browInnerUp + eyeWideLeft + eyeWideRight + jawOpen) / 4.0

        // ÊÇ≤„Åó„Åø„ÅÆÊ§úÂá∫
        let mouthLowerDownLeft = blendShapes[.mouthLowerDownLeft]?.floatValue ?? 0.0
        let mouthLowerDownRight = blendShapes[.mouthLowerDownRight]?.floatValue ?? 0.0
        let browOuterUpLeft = blendShapes[.browOuterUpLeft]?.floatValue ?? 0.0
        let browOuterUpRight = blendShapes[.browOuterUpRight]?.floatValue ?? 0.0
        let sadnessIntensity =
            (mouthLowerDownLeft + mouthLowerDownRight + browOuterUpLeft + browOuterUpRight) / 4.0

        // ÊúÄ„ÇÇÂº∑„ÅÑË°®ÊÉÖ„ÇíÊ±∫ÂÆö
        let intensities: [(expression: FacialExpression, intensity: Float)] = [
            (.happy, smileIntensity),
            (.angry, angerIntensity),
            (.surprised, surpriseIntensity),
            (.sad, sadnessIntensity),
        ]

        let strongestExpression = intensities.max { $0.intensity < $1.intensity }

        // ÈñæÂÄ§„ÇíË®≠ÂÆöÔºà0.3‰ª•‰∏ä„ÅßË°®ÊÉÖ„Å®„Åó„Å¶Ë™çË≠òÔºâ
        let threshold: Float = 0.3

        if let strongest = strongestExpression, strongest.intensity > threshold {
            return (strongest.expression, strongest.intensity)
        } else {
            return (.neutral, 0.8)  // ‰∏≠Á´ãË°®ÊÉÖ
        }
    }

    // MARK: - Simulation Mode (Fallback)

    private func startSimulation() {
        simulationTimer = Timer.scheduledTimer(withTimeInterval: 2.0, repeats: true) {
            [weak self] _ in
            self?.simulateExpressionChange()
        }
    }

    private func simulateExpressionChange() {
        let expressions: [FacialExpression] = [.neutral, .happy, .angry, .surprised, .sad]
        let randomExpression = expressions.randomElement() ?? .neutral
        let randomConfidence = Float.random(in: 0.3...0.9)

        DispatchQueue.main.async {
            self.currentExpression = randomExpression
            self.confidence = randomConfidence

            let result = FacialExpressionResult(
                expression: randomExpression,
                confidence: randomConfidence,
                timestamp: Date()
            )

            self.delegate?.facialExpressionManager(self, didDetectExpression: result)
        }
    }
}

// MARK: - ARSessionDelegate

extension FacialExpressionManager: ARSessionDelegate {
    func session(_ session: ARSession, didAdd anchors: [ARAnchor]) {
        for anchor in anchors {
            if anchor is ARFaceAnchor {
                DispatchQueue.main.async {
                    self.isFaceDetected = true
                }
                print("FacialExpressionManager: Face detected")
            }
        }
    }

    func session(_ session: ARSession, didUpdate anchors: [ARAnchor]) {
        for anchor in anchors {
            guard let faceAnchor = anchor as? ARFaceAnchor else { continue }

            let blendShapes = faceAnchor.blendShapes
            let (expression, confidence) = analyzeBlendShapes(blendShapes)

            DispatchQueue.main.async {
                self.currentExpression = expression
                self.confidence = confidence
                self.isFaceDetected = true

                let result = FacialExpressionResult(
                    expression: expression,
                    confidence: confidence,
                    timestamp: Date()
                )

                self.delegate?.facialExpressionManager(self, didDetectExpression: result)
            }
        }
    }

    func session(_ session: ARSession, didRemove anchors: [ARAnchor]) {
        for anchor in anchors {
            if anchor is ARFaceAnchor {
                DispatchQueue.main.async {
                    self.isFaceDetected = false
                    self.currentExpression = .neutral
                    self.confidence = 0.0
                }
                print("FacialExpressionManager: Face lost")
            }
        }
    }

    func session(_ session: ARSession, didFailWithError error: Error) {
        print("FacialExpressionManager: ARSession failed with error: \(error)")

        DispatchQueue.main.async {
            self.isTracking = false
            self.isFaceDetected = false
        }

        // „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„Å®„Åó„Å¶„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„É¢„Éº„Éâ„Å´Âàá„ÇäÊõø„Åà
        if !isARKitSupported {
            startSimulation()
        }
    }

    func sessionWasInterrupted(_ session: ARSession) {
        print("FacialExpressionManager: ARSession was interrupted")
        DispatchQueue.main.async {
            self.isTracking = false
        }
    }

    func sessionInterruptionEnded(_ session: ARSession) {
        print("FacialExpressionManager: ARSession interruption ended")
        DispatchQueue.main.async {
            self.isTracking = true
        }
    }
}
