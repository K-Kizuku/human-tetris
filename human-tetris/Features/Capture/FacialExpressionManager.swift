//
//  FacialExpressionManager.swift
//  human-tetris
//
//  Created by Kiro on 2025/08/17.
//

import ARKit
import Combine
import SwiftUI

// è¡¨æƒ…ã®ç¨®é¡ã‚’å®šç¾©
enum FacialExpression: String, CaseIterable {
    case neutral = "ä¸­ç«‹"
    case happy = "å–œã³"
    case angry = "æ€’ã‚Š"
    case surprised = "é©šã"
    case sad = "æ‚²ã—ã¿"

    var emoji: String {
        switch self {
        case .neutral: return "ğŸ˜"
        case .happy: return "ğŸ˜Š"
        case .angry: return "ğŸ˜ "
        case .surprised: return "ğŸ˜²"
        case .sad: return "ğŸ˜¢"
        }
    }

    /// è¡¨æƒ…ã«å¿œã˜ãŸãƒ†ãƒˆãƒªã‚¹è½ä¸‹é–“éš”ã®å€ç‡
    /// ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…ã»ã©é…ãï¼ˆé–“éš”ã‚’é•·ãï¼‰ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…ã»ã©é€Ÿãï¼ˆé–“éš”ã‚’çŸ­ãï¼‰
    var dropSpeedMultiplier: Double {
        switch self {
        case .happy: return 1.25  // å–œã³ï¼šã‚†ã£ãã‚Šè½ä¸‹ï¼ˆé–“éš”ã‚’é•·ãï¼‰
        case .neutral: return 1.0  // ä¸­ç«‹ï¼šé€šå¸¸é€Ÿåº¦
        case .surprised: return 0.83  // é©šãï¼šå°‘ã—é€Ÿãï¼ˆé–“éš”ã‚’çŸ­ãï¼‰
        case .sad: return 0.77  // æ‚²ã—ã¿ï¼šé€Ÿãï¼ˆé–“éš”ã‚’çŸ­ãï¼‰
        case .angry: return 0.67  // æ€’ã‚Šï¼šæœ€ã‚‚é€Ÿãï¼ˆé–“éš”ã‚’çŸ­ãï¼‰
        }
    }

    /// è¡¨æƒ…ã®æ„Ÿæƒ…çš„ãªæ¥µæ€§ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
    var emotionalPolarity: String {
        switch self {
        case .happy: return "ãƒã‚¸ãƒ†ã‚£ãƒ–"
        case .neutral: return "ä¸­ç«‹"
        case .surprised: return "ä¸­ç«‹"
        case .sad: return "ãƒã‚¬ãƒ†ã‚£ãƒ–"
        case .angry: return "ãƒã‚¬ãƒ†ã‚£ãƒ–"
        }
    }
}

// è¡¨æƒ…èªè­˜ã®çµæœã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
struct FacialExpressionResult {
    let expression: FacialExpression
    let confidence: Float
    let timestamp: Date
}

protocol FacialExpressionManagerDelegate {
    func facialExpressionManager(
        _ manager: FacialExpressionManager, didDetectExpression result: FacialExpressionResult)
    func facialExpressionManager(_ manager: FacialExpressionManager, didEncounterError error: Error)
    // çµ±åˆARSessionã‹ã‚‰èƒŒé¢ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æä¾›
    func facialExpressionManager(
        _ manager: FacialExpressionManager, didOutputBackCameraFrame pixelBuffer: CVPixelBuffer)
}

class FacialExpressionManager: NSObject, ObservableObject {
    @Published var currentExpression: FacialExpression = .neutral
    @Published var confidence: Float = 0.0
    @Published var isTracking = false
    @Published var isFaceDetected = false
    @Published var isARKitSupported = false
    @Published var currentBackCameraFrame: CVPixelBuffer?

    var delegate: FacialExpressionManagerDelegate?

    private let sessionQueue = DispatchQueue(label: "facial.expression.session.queue")
    private var simulationTimer: Timer?

    // ARKité–¢é€£
    private var arSession: ARSession?
    private var arConfiguration: ARFaceTrackingConfiguration?
    
    // è¡¨æƒ…èªè­˜æ›´æ–°é »åº¦åˆ¶é™ï¼ˆ1ç§’ã«2å› = 500msé–“éš”ï¼‰
    private var lastExpressionUpdateTime: Date = Date.distantPast
    private let minUpdateInterval: TimeInterval = 0.5  // 500ms
    
    // èƒŒé¢ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ é…ä¿¡ç”¨ã®é »åº¦åˆ¶é™ï¼ˆ30fps -> 15fps ã«èª¿æ•´ï¼‰
    private var lastBackCameraFrameTime: Date = Date.distantPast
    private let backCameraFrameInterval: TimeInterval = 1.0 / 15.0  // 15fps

    override init() {
        super.init()
        checkARKitSupport()
        print("FacialExpressionManager: Initialized with ARKit support: \(isARKitSupported)")
    }

    deinit {
        stopTracking()
    }

    private func checkARKitSupport() {
        isARKitSupported = ARFaceTrackingConfiguration.isSupported
        print("FacialExpressionManager: ARKit Face Tracking supported: \(isARKitSupported)")
    }

    func startTracking() {
        print("FacialExpressionManager: Starting ARKit based tracking")

        sessionQueue.async { [weak self] in
            guard let self = self else { return }

            #if targetEnvironment(simulator)
            print("FacialExpressionManager: Running in simulator, using simulation mode")
            DispatchQueue.main.async {
                self.startSimulation()
            }
            #else
            if self.isARKitSupported {
                print("FacialExpressionManager: Attempting ARKit tracking")
                
                // ARKitãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚’é–‹å§‹
                self.startARKitTracking()
                
                // 5ç§’å¾Œã«ARKitãŒå‹•ä½œã—ã¦ã„ãªã„å ´åˆã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ
                DispatchQueue.main.asyncAfter(deadline: .now() + 5.0) {
                    if !self.isFaceDetected && self.isTracking {
                        print("FacialExpressionManager: ARKit timeout, falling back to simulation")
                        self.arSession?.pause()
                        self.arSession = nil
                        self.startSimulation()
                    }
                }
            } else {
                print("FacialExpressionManager: ARKit not supported, using simulation mode")
                DispatchQueue.main.async {
                    self.startSimulation()
                }
            }
            #endif
        }
    }

    func stopTracking() {
        print("FacialExpressionManager: Stopping tracking")

        sessionQueue.async { [weak self] in
            // ARKitã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åœæ­¢
            self?.arSession?.pause()
            self?.arSession = nil

            // ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢
            self?.simulationTimer?.invalidate()
            self?.simulationTimer = nil

            DispatchQueue.main.async {
                self?.isTracking = false
                self?.isFaceDetected = false
                self?.currentExpression = .neutral
                self?.confidence = 0.0
            }
        }
    }

    // MARK: - ARKit Face Tracking

    private func startARKitTracking() {
        guard ARWorldTrackingConfiguration.isSupported else {
            print("FacialExpressionManager: ARKit World Tracking not supported")
            DispatchQueue.main.async {
                self.startSimulation()
            }
            return
        }

        // ARKitä¸€æœ¬åŒ–: ARWorldTrackingConfigurationã§userFaceTrackingEnabledã‚’ä½¿ç”¨
        let configuration = ARWorldTrackingConfiguration()
        
        // Appleå…¬å¼ã®æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: èƒŒé¢ï¼ˆãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼‰+ å‰é¢ï¼ˆãƒ•ã‚§ã‚¤ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼‰ã‚’åŒä¸€ARSessionã§
        if #available(iOS 13.0, *) {
            configuration.userFaceTrackingEnabled = true
            print("FacialExpressionManager: Using ARWorldTrackingConfiguration with userFaceTrackingEnabled")
        } else {
            // iOS 13æœªæº€ã§ã¯å¾“æ¥ã®ARFaceTrackingConfigurationã‚’ä½¿ç”¨
            print("FacialExpressionManager: iOS 13æœªæº€ã®ãŸã‚ã€ARFaceTrackingConfigurationã‚’ä½¿ç”¨")
            let faceConfig = ARFaceTrackingConfiguration()
            faceConfig.maximumNumberOfTrackedFaces = 1
            
            arSession = ARSession()
            arSession?.delegate = self
            arSession?.delegateQueue = DispatchQueue(label: "arkit.session.queue", qos: .userInitiated)
            
            DispatchQueue.global(qos: .userInitiated).async {
                self.arSession?.run(faceConfig, options: [.resetTracking, .removeExistingAnchors])
                DispatchQueue.main.async {
                    self.isTracking = true
                    print("FacialExpressionManager: ARFaceTrackingConfiguration session started")
                }
            }
            return
        }
        
        // planeDetectionã¯ç„¡åŠ¹åŒ–ã—ã¦ãƒªã‚½ãƒ¼ã‚¹ç¯€ç´„
        configuration.planeDetection = []
        
        // ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã‚’æœ€é©åŒ–
        if let videoFormat = ARWorldTrackingConfiguration.supportedVideoFormats.first(where: { format in
            format.framesPerSecond == 30 && format.imageResolution.width <= 1280
        }) {
            configuration.videoFormat = videoFormat
            print("FacialExpressionManager: Using optimized video format: \(videoFormat.imageResolution)@\(videoFormat.framesPerSecond)fps")
        }

        arSession = ARSession()
        arSession?.delegate = self
        arSession?.delegateQueue = DispatchQueue(label: "unified.arkit.session.queue", qos: .userInitiated)

        print("FacialExpressionManager: Starting unified ARKit session (World + Face tracking)")
        
        DispatchQueue.global(qos: .userInitiated).async {
            let options: ARSession.RunOptions = [.resetTracking, .removeExistingAnchors]
            self.arSession?.run(configuration, options: options)
            DispatchQueue.main.async {
                self.isTracking = true
                print("FacialExpressionManager: Unified ARKit session started successfully")
            }
        }
    }

    private func analyzeBlendShapes(_ blendShapes: [ARFaceAnchor.BlendShapeLocation: NSNumber]) -> (
        expression: FacialExpression, confidence: Float
    ) {
        // å„è¡¨æƒ…ã®ç‰¹å¾´çš„ãªblendShapeã‚’åˆ†æ

        // ç¬‘é¡”ã®æ¤œå‡ºï¼ˆã‚ˆã‚Šæ•æ„Ÿã«ï¼‰
        let mouthSmileLeft = blendShapes[.mouthSmileLeft]?.floatValue ?? 0.0
        let mouthSmileRight = blendShapes[.mouthSmileRight]?.floatValue ?? 0.0
        let cheekPuff = blendShapes[.cheekPuff]?.floatValue ?? 0.0
        let mouthDimpleLeft = blendShapes[.mouthDimpleLeft]?.floatValue ?? 0.0
        let mouthDimpleRight = blendShapes[.mouthDimpleRight]?.floatValue ?? 0.0
        
        // è¤‡æ•°ã®ç¬‘é¡”æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›ã¦é©åº¦ãªæ„Ÿåº¦å‘ä¸Š
        let baseSmile = (mouthSmileLeft + mouthSmileRight) / 2.0
        let additionalSmileFeatures = (cheekPuff + mouthDimpleLeft + mouthDimpleRight) / 3.0
        let smileIntensity = baseSmile + (additionalSmileFeatures * 0.3)  // è¿½åŠ ç‰¹å¾´é‡ã‚’30%ã®é‡ã¿ã§åŠ ç®—ï¼ˆä¸­é–“ãƒ¬ãƒ™ãƒ«ï¼‰
        
        // ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šã‚¹ãƒã‚¤ãƒ«æ¤œå‡ºã®è©³ç´°ï¼ˆé–¾å€¤è¿‘è¾ºã®ã¿ï¼‰
        if smileIntensity > 0.10 {
            print("FacialExpressionManager: Smile detection - base: \(String(format: "%.3f", baseSmile)), additional: \(String(format: "%.3f", additionalSmileFeatures)), total: \(String(format: "%.3f", smileIntensity))")
        }

        // æ€’ã‚Šã®æ¤œå‡º
        let browDownLeft = blendShapes[.browDownLeft]?.floatValue ?? 0.0
        let browDownRight = blendShapes[.browDownRight]?.floatValue ?? 0.0
        let mouthFrownLeft = blendShapes[.mouthFrownLeft]?.floatValue ?? 0.0
        let mouthFrownRight = blendShapes[.mouthFrownRight]?.floatValue ?? 0.0
        let angerIntensity = (browDownLeft + browDownRight + mouthFrownLeft + mouthFrownRight) / 4.0

        // é©šãã®æ¤œå‡º
        let browInnerUp = blendShapes[.browInnerUp]?.floatValue ?? 0.0
        let eyeWideLeft = blendShapes[.eyeWideLeft]?.floatValue ?? 0.0
        let eyeWideRight = blendShapes[.eyeWideRight]?.floatValue ?? 0.0
        let jawOpen = blendShapes[.jawOpen]?.floatValue ?? 0.0
        let surpriseIntensity = (browInnerUp + eyeWideLeft + eyeWideRight + jawOpen) / 4.0

        // æ‚²ã—ã¿ã®æ¤œå‡º
        let mouthLowerDownLeft = blendShapes[.mouthLowerDownLeft]?.floatValue ?? 0.0
        let mouthLowerDownRight = blendShapes[.mouthLowerDownRight]?.floatValue ?? 0.0
        let browOuterUpLeft = blendShapes[.browOuterUpLeft]?.floatValue ?? 0.0
        let browOuterUpRight = blendShapes[.browOuterUpRight]?.floatValue ?? 0.0
        let sadnessIntensity =
            (mouthLowerDownLeft + mouthLowerDownRight + browOuterUpLeft + browOuterUpRight) / 4.0

        // æœ€ã‚‚å¼·ã„è¡¨æƒ…ã‚’æ±ºå®š
        let intensities: [(expression: FacialExpression, intensity: Float)] = [
            (.happy, smileIntensity),
            (.angry, angerIntensity),
            (.surprised, surpriseIntensity),
            (.sad, sadnessIntensity),
        ]

        let strongestExpression = intensities.max { $0.intensity < $1.intensity }

        // é–¾å€¤ã‚’è¨­å®šï¼ˆä¸­é–“ãƒ¬ãƒ™ãƒ«ï¼šé©åº¦ãªæ„Ÿåº¦ã§è¡¨æƒ…èªè­˜ï¼‰
        let generalThreshold: Float = 0.22  // å…ƒ0.3ã¨ç¾0.15ã®ä¸­é–“
        let smileThreshold: Float = 0.12    // å…ƒ0.3ã¨ç¾0.08ã®ä¸­é–“ãƒ¬ãƒ™ãƒ«

        if let strongest = strongestExpression {
            let applicableThreshold = (strongest.expression == .happy) ? smileThreshold : generalThreshold
            
            if strongest.intensity > applicableThreshold {
                // ä¿¡é ¼åº¦ã‚’å‘ä¸Šã•ã›ã‚‹è¨ˆç®—ï¼šã‚ˆã‚Šé«˜ã„å€¤ã‚’å‡ºåŠ›
                var enhancedConfidence: Float
                
                if strongest.expression == .happy {
                    // ã‚¹ãƒã‚¤ãƒ«ã®å ´åˆã¯é©åº¦ã«æ„Ÿåº¦ã‚’ä¸Šã’ã‚‹ï¼ˆä¸­é–“ãƒ¬ãƒ™ãƒ«ï¼‰
                    enhancedConfidence = min(1.0, strongest.intensity * 2.0 + 0.2)  // ã‚¹ãƒã‚¤ãƒ«ã¯2.2å€ï¼‹0.2ã®ãƒ™ãƒ¼ã‚¹å€¤
                } else {
                    enhancedConfidence = min(1.0, strongest.intensity * 2.0 + 0.2)   // ãã®ä»–ã¯2.0å€ï¼‹0.2ã®ãƒ™ãƒ¼ã‚¹å€¤
                }
                
                return (strongest.expression, enhancedConfidence)
            } else {
                return (.neutral, 0.8)  // ä¸­ç«‹è¡¨æƒ…
            }
        } else {
            return (.neutral, 0.8)  // è¡¨æƒ…ãŒæ¤œå‡ºã•ã‚Œãªã„å ´åˆ
        }
    }

    // MARK: - Simulation Mode (Fallback)

    private func startSimulation() {
        print("FacialExpressionManager: Starting simulation mode")
        DispatchQueue.main.async {
            self.isTracking = true
            self.isFaceDetected = true
            
            // æœ€åˆã®è¡¨æƒ…ã‚’å³åº§ã«è¨­å®š
            self.simulateExpressionChange()
            
            // å®šæœŸçš„ãªè¡¨æƒ…å¤‰åŒ–ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹
            self.simulationTimer = Timer.scheduledTimer(withTimeInterval: 3.0, repeats: true) {
                [weak self] _ in
                self?.simulateExpressionChange()
            }
        }
    }

    private func simulateExpressionChange() {
        let expressions: [FacialExpression] = [.neutral, .happy, .angry, .surprised, .sad]
        let randomExpression = expressions.randomElement() ?? .neutral
        let randomConfidence = Float.random(in: 0.3...0.9)

        DispatchQueue.main.async {
            self.currentExpression = randomExpression
            self.confidence = randomConfidence

            let result = FacialExpressionResult(
                expression: randomExpression,
                confidence: randomConfidence,
                timestamp: Date()
            )

            self.delegate?.facialExpressionManager(self, didDetectExpression: result)
        }
    }
}

// MARK: - ARSessionDelegate

extension FacialExpressionManager: ARSessionDelegate {
    func session(_ session: ARSession, didUpdate frame: ARFrame) {
        // ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆèª¿æ•´: 15fpsã«åˆ¶é™ã—ã¦å‡¦ç†è² è·ã‚’è»½æ¸›
        let now = Date()
        guard now.timeIntervalSince(lastBackCameraFrameTime) >= backCameraFrameInterval else {
            return
        }
        lastBackCameraFrameTime = now
        
        // çµ±åˆARSessionã‹ã‚‰èƒŒé¢ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
        let pixelBuffer = frame.capturedImage
        
        // UIã‚¹ãƒ¬ãƒƒãƒ‰ã§ç¾åœ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ›´æ–°ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºç”¨ï¼‰
        DispatchQueue.main.async { [weak self] in
            self?.currentBackCameraFrame = pixelBuffer
        }
        
        // Visionå‡¦ç†ã®ãŸã‚ã«ãƒ‡ãƒªã‚²ãƒ¼ãƒˆã«ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é€ä¿¡
        delegate?.facialExpressionManager(self, didOutputBackCameraFrame: pixelBuffer)
    }
    
    func session(_ session: ARSession, didAdd anchors: [ARAnchor]) {
        for anchor in anchors {
            if anchor is ARFaceAnchor {
                DispatchQueue.main.async {
                    self.isFaceDetected = true
                }
                print("FacialExpressionManager: Face detected via unified ARSession")
            }
        }
    }

    func session(_ session: ARSession, didUpdate anchors: [ARAnchor]) {
        // æ›´æ–°é »åº¦åˆ¶é™ï¼š1ç§’ã«2å›ã¾ã§ï¼ˆ500msé–“éš”ï¼‰
        let now = Date()
        guard now.timeIntervalSince(lastExpressionUpdateTime) >= minUpdateInterval else {
            return // ã‚¹ã‚­ãƒƒãƒ—ã—ã¦èƒŒé¢ã‚«ãƒ¡ãƒ©ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¯€ç´„
        }
        
        for anchor in anchors {
            guard let faceAnchor = anchor as? ARFaceAnchor else { continue }

            let blendShapes = faceAnchor.blendShapes
            let (expression, confidence) = analyzeBlendShapes(blendShapes)

            print("FacialExpressionManager: Detected expression: \(expression.rawValue) with confidence: \(confidence)")

            // æ›´æ–°æ™‚åˆ»ã‚’è¨˜éŒ²
            lastExpressionUpdateTime = now

            DispatchQueue.main.async {
                self.currentExpression = expression
                self.confidence = confidence
                self.isFaceDetected = true

                let result = FacialExpressionResult(
                    expression: expression,
                    confidence: confidence,
                    timestamp: Date()
                )

                self.delegate?.facialExpressionManager(self, didDetectExpression: result)
            }
            
            // ä¸€ã¤ã®ã‚¢ãƒ³ã‚«ãƒ¼ã®ã¿å‡¦ç†ã—ã¦ãƒªã‚½ãƒ¼ã‚¹ç¯€ç´„
            break
        }
    }

    func session(_ session: ARSession, didRemove anchors: [ARAnchor]) {
        for anchor in anchors {
            if anchor is ARFaceAnchor {
                DispatchQueue.main.async {
                    self.isFaceDetected = false
                    self.currentExpression = .neutral
                    self.confidence = 0.0
                }
                print("FacialExpressionManager: Face lost")
            }
        }
    }

    func session(_ session: ARSession, didFailWithError error: Error) {
        print("FacialExpressionManager: ARSession failed with error: \(error)")
        
        // ã‚«ãƒ¡ãƒ©ãƒªã‚½ãƒ¼ã‚¹ã®ç«¶åˆã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
        if let arError = error as? ARError {
            switch arError.code {
            case .cameraUnauthorized:
                print("FacialExpressionManager: Camera unauthorized")
            case .unsupportedConfiguration:
                print("FacialExpressionManager: Unsupported configuration")
            case .invalidReferenceImage:
                print("FacialExpressionManager: Invalid reference image")
            default:
                print("FacialExpressionManager: ARError: \(arError.localizedDescription)")
            }
        }

        DispatchQueue.main.async {
            self.isTracking = false
            self.isFaceDetected = false
        }

        // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å¸¸ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ
        print("FacialExpressionManager: Falling back to simulation mode due to error")
        DispatchQueue.main.async {
            self.startSimulation()
        }
    }

    func sessionWasInterrupted(_ session: ARSession) {
        print("FacialExpressionManager: ARSession was interrupted")
        DispatchQueue.main.async {
            self.isTracking = false
        }
    }

    func sessionInterruptionEnded(_ session: ARSession) {
        print("FacialExpressionManager: ARSession interruption ended")
        DispatchQueue.main.async {
            self.isTracking = true
        }
    }
}
